# TTT-Discover unified configuration
#
# This file contains:
# 1) active settings for this repository's TTT runner, and
# 2) a reference block mirroring mlx-lm/examples/lora_config.yaml
#    for LoRA hyperparameter parity.

experiment:
  name: ttt_discover_gsm8k_mvp
debug: false

# -----------------------------
# Active TTT settings (this repo)
# -----------------------------
data:
  split: test
  # max_examples: 64
  max_examples: 1319
  # local_cache_path: tests/fixtures/gsm8k_tiny.jsonl
  local_cache_path: null

sampling:
  temperature: 1.0
  top_p: 1.0
  top_k: 0
  max_new_tokens: 256
  seed: 0

grouping:
  G: 8
  #G: 2
  B: 64
  #B: 8

reuse:
  c: 1.0
  max_archive_size: 1000
  top_children_per_parent: 2

loss:
  beta: 2.0
  beta_mode: constant # constant | adaptive_kl
  gamma: 0.6931471805599453 # ln(2)
  lambda_kl: 0.1
  lr: 1.0e-5
  steps_per_outer: 1

backend:
  name: mlx # stub | mlx
  # model_name_or_path: mlx-community/Llama-3.2-1B-Instruct-bf16
  model_name_or_path: mlx-community/Falcon-H1-Tiny-90M-Instruct-4bit
  lora_rank: 8
  quantized: false
  trust_remote_code: true
  debug: false

train:
  outer_steps: 20
  init_rollouts_per_example: 2

benchmark:
  after_hotswap: true
  max_examples: 64
  temperature: 0.0
  max_new_tokens: 256

checkpoint:
  after_outer_step: true
  dir: checkpoints

# ---------------------------------------------
# Reference parity block from mlx-lm LoRA config
# ---------------------------------------------
mlx_lm_lora_reference:
  model: mlx-community/Llama-3.2-1B-Instruct-bf16
  train: true
  fine_tune_type: lora

  optimizer: adamw
  optimizer_config:
    adamw:
      betas: [0.9, 0.98]
      eps: 1.0e-6
      weight_decay: 0.05
      bias_correction: true

  data: mlx-community/WikiSQL
  seed: 0
  num_layers: 16
  batch_size: 4
  iters: 1000
  val_batches: 25
  learning_rate: 1.0e-5

  steps_per_report: 10
  steps_per_eval: 200
  grad_accumulation_steps: 1

  resume_adapter_file: null
  adapter_path: adapters
  save_every: 100

  test: false
  test_batches: 100
  max_seq_length: 2048
  grad_checkpoint: false

  lora_parameters:
    keys: ["self_attn.q_proj", "self_attn.v_proj"]
    rank: 8
    scale: 20.0
    dropout: 0.0

  lr_schedule: null
  hf_dataset: null
